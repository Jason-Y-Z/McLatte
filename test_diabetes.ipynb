{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "from mclatte.model import (\n",
    "    train_mclatte, \n",
    "    train_semi_skimmed_mclatte, \n",
    "    train_skimmed_mclatte, \n",
    "    McLatte,\n",
    "    SemiSkimmedMcLatte,\n",
    "    SkimmedMcLatte,\n",
    ")\n",
    "from rnn.model import (\n",
    "    train_baseline_rnn,\n",
    "    BaselineRnn,\n",
    ")\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.preprocessing import scale\n",
    "from synctwin.model import (\n",
    "    train_synctwin,\n",
    "    SyncTwinPl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa0b861d8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(509)\n",
    "np.random.seed(509)\n",
    "torch.manual_seed(509)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBJECTS = 70\n",
    "M = 5\n",
    "H = 5\n",
    "R = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CODES = {\n",
    "    33: 'reg_insulin',          # treatment\n",
    "    34: 'nph_insulin',          # treatment\n",
    "    35: 'ult_insulin',          # treatment\n",
    "    48: 'unspecified_bg',       # outcome\n",
    "    57: 'unspecified_bg',       # outcome\n",
    "    58: 'pre_breakfast_bg',     # outcome\n",
    "    59: 'post_breakfast_bg',    # outcome\n",
    "    60: 'pre_lunch_bg',         # outcome\n",
    "    61: 'post_lunch_bg',        # outcome\n",
    "    62: 'pre_supper_bg',        # outcome\n",
    "    63: 'post_supper_bg',       # outcome\n",
    "    64: 'pre_snack_bg',         # outcome\n",
    "    65: 'hypo_symptoms',        # covariate\n",
    "    66: 'typical_meal',         # covariate\n",
    "    67: 'more_meal',            # covariate\n",
    "    68: 'less_meal',            # covariate\n",
    "    69: 'typical_exercise',     # covariate\n",
    "    70: 'more_exercise',        # covariate\n",
    "    71: 'less_exercise',        # covariate\n",
    "    72: 'unspecified_event',    # covariate\n",
    "}\n",
    "TREATMENT_COLS = ['reg_insulin', 'nph_insulin', 'ult_insulin']\n",
    "OUTCOME_COLS = ['unspecified_bg', 'pre_breakfast_bg', 'post_breakfast_bg', 'pre_lunch_bg', 'post_lunch_bg', 'pre_supper_bg', 'post_supper_bg', 'pre_snack_bg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_or_na(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except Exception as e:\n",
    "        if v == '0Hi':\n",
    "            return 1\n",
    "        if v == '0Lo':\n",
    "            return -1\n",
    "        print(e)\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(values):\n",
    "    valid_values = values[pd.notna(values)]\n",
    "    if valid_values.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return np.median(valid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_date(v):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(v, '%m-%d-%Y')\n",
    "    except Exception as e:\n",
    "        print(f'{e}: {v}')\n",
    "    try:\n",
    "        v = v[:4] + '0' + v[5:]  # handle date mis-input (e.g. 6-31)\n",
    "        return datetime.datetime.strptime(v, '%m-%d-%Y')\n",
    "    except Exception as e:\n",
    "        print(f'{e}: {v}')\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_time(v):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(v, '%H:%M').time()\n",
    "    except Exception as e:\n",
    "        print(f'{e}: {v}')\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_combine(date, time):\n",
    "    try:\n",
    "        return datetime.datetime.combine(date, time)\n",
    "    except Exception as e:\n",
    "        print(f'{e}: {date} {time}')\n",
    "    if isinstance(date, datetime.datetime):\n",
    "        return date\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_i(subject_idx):\n",
    "    raw_df = pd.read_csv(os.path.join(os.getcwd(), f'data/diabetes/data-{subject_idx:02d}'), sep='\\t', names=['date', 'time', 'code', 'value'])\n",
    "    raw_df['date'] = raw_df['date'].apply(try_to_date)\n",
    "    raw_df['time'] = raw_df['time'].apply(try_to_time)\n",
    "    raw_df['datetime'] = raw_df.apply(lambda row: try_to_combine(row['date'], row['time']), axis=1)\n",
    "    raw_df.drop(columns=['date', 'time'], inplace=True)\n",
    "    raw_df.sort_values(by=['datetime'], inplace=True)\n",
    "    \n",
    "    all_datetimes = raw_df.datetime.values\n",
    "    converted_df = pd.DataFrame(index=range(len(set(all_datetimes))), columns=list(DATA_CODES.values()))\n",
    "\n",
    "    begin_idx = 0\n",
    "    converted_idx = 0\n",
    "    while begin_idx < raw_df.shape[0]:\n",
    "        while begin_idx < raw_df.shape[0] and np.isnan(all_datetimes[begin_idx]):\n",
    "            begin_idx += 1\n",
    "        \n",
    "        end_idx = begin_idx\n",
    "        while end_idx < raw_df.shape[0] and all_datetimes[end_idx] == all_datetimes[begin_idx]:\n",
    "            if raw_df.iloc[end_idx]['code'] in DATA_CODES:\n",
    "                col_name = DATA_CODES[raw_df.iloc[end_idx]['code']]\n",
    "                converted_df.iloc[converted_idx][col_name] = float_or_na(raw_df.iloc[end_idx]['value'])\n",
    "            end_idx += 1\n",
    "        begin_idx = end_idx\n",
    "        converted_idx += 1\n",
    "\n",
    "    outcomes = converted_df.apply(lambda row: combine(row[OUTCOME_COLS]), axis=1)\n",
    "    treatment = converted_df[TREATMENT_COLS].apply(lambda col: combine(col), axis=0)\n",
    "    converted_df = converted_df[TREATMENT_COLS + OUTCOME_COLS]\n",
    "\n",
    "    mask_df = ~converted_df.isna()\n",
    "    converted_df[pd.isna(converted_df)] = 0\n",
    "    treatment[pd.isna(treatment)] = 0\n",
    "    return (\n",
    "        converted_df.to_numpy(), \n",
    "        mask_df.to_numpy(), \n",
    "        outcomes[pd.notna(outcomes)].to_numpy(), \n",
    "        treatment.to_numpy(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Initialisation\n",
    "    X = []\n",
    "    M_ = []\n",
    "    Y_pre = []\n",
    "    Y_post = []\n",
    "    A = []\n",
    "\n",
    "    # Reading\n",
    "    for subject_idx in range(1, N_SUBJECTS + 1):\n",
    "        X_i, M_i, Y_i, A_i = load_subject_i(subject_idx)\n",
    "        \n",
    "        if M * R > M_i.shape[0]:\n",
    "            M_.append(np.concatenate((np.zeros((M * R - M_i.shape[0], M_i.shape[1])), M_i)))\n",
    "        else:\n",
    "            M_.append(M_i[-M * R:])\n",
    "        \n",
    "        if H + M > Y_i.shape[0]:\n",
    "            Y_pre.append(np.concatenate((np.zeros(H + M - Y_i.shape[0]), Y_i[:-H])))\n",
    "        else:\n",
    "            Y_pre.append(Y_i[-(H + M):-H])\n",
    "        \n",
    "        Y_post.append(Y_i[-H:])\n",
    "        A.append(A_i)\n",
    "        X_i = X_i[:-H]\n",
    "        if M * R > X_i.shape[0]:\n",
    "            X.append(np.concatenate((np.zeros((M * R - X_i.shape[0], X_i.shape[1])), X_i)))\n",
    "        else:\n",
    "            X.append(X_i[-M * R:])\n",
    "\n",
    "    # Aggregation\n",
    "    X = np.stack(X)\n",
    "    M_ = np.stack(M_)\n",
    "    Y_pre = np.array(Y_pre)\n",
    "    Y_post = np.array(Y_post)\n",
    "    A = np.array(A)\n",
    "    T = np.transpose(np.tile(np.arange(-M * R, 0), (N_SUBJECTS, X.shape[2], 1)), (0, 2, 1))\n",
    "    \n",
    "    # Scaling\n",
    "    X_to_scale = X.reshape((-1, X.shape[2]))  # (N, T, D) -> (N * T, D)\n",
    "    X_scaled = scale(X_to_scale, axis=0)\n",
    "    X = X_scaled.reshape(X.shape)\n",
    "    \n",
    "    Y = np.concatenate((Y_pre, Y_post), axis=1)\n",
    "    Y_to_scale = Y.reshape((-1, 1))  # (N, M) + (N, H) -> (N * T, 1)\n",
    "    Y_scaled = scale(Y_to_scale, axis=0)\n",
    "    Y = Y_scaled.reshape(Y.shape)\n",
    "    Y_pre, Y_post = Y[:, :-H], Y[:, -H:]\n",
    "    \n",
    "    A = scale(A, axis=0)  # [N, K]\n",
    "    \n",
    "    return X, M_, Y_pre, Y_post, A, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_write_data():\n",
    "    X, M_, Y_pre, Y_post, A, T = load_data()\n",
    "    joblib.dump((X, M_, Y_pre, Y_post, A, T), os.path.join(os.getcwd(), 'data/diabetes/processed.joblib'))\n",
    "    \n",
    "    N = N_SUBJECTS\n",
    "    D = X.shape[2] \n",
    "    K = A.shape[1] \n",
    "    C = 4 \n",
    "    joblib.dump(\n",
    "        (N, M, H, R, D, K, C, X, M_, Y_pre, Y_post, A, T),\n",
    "        os.path.join(os.getcwd(), f\"data/diabetes/hp_search.joblib\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    N_train = round(N_SUBJECTS * 0.8)\n",
    "    N_test = round(N_SUBJECTS * 0.2)\n",
    "    X, M_, Y_pre, Y_post, A, T = joblib.load(os.path.join(os.getcwd(), 'data/diabetes/processed.joblib'))\n",
    "    X_train, X_test = X[:N_train], X[N_train:]\n",
    "    M_train, M_test = M_[:N_train], M_[N_train:]\n",
    "    Y_pre_train, Y_pre_test = Y_pre[:N_train], Y_pre[N_train:]\n",
    "    Y_post_train, Y_post_test = Y_post[:N_train], Y_post[N_train:]\n",
    "    A_train, A_test = A[:N_train], A[N_train:]\n",
    "    T_train, T_test = T[:N_train], T[N_train:]\n",
    "    all_data = (\n",
    "        N_SUBJECTS, N_train, N_test, \n",
    "        X_train, X_test, \n",
    "        M_train, M_test, \n",
    "        Y_pre_train, Y_pre_test, \n",
    "        Y_post_train, Y_post_test, \n",
    "        A_train, A_test, \n",
    "        T_train, T_test\n",
    "    )\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M, H, R, D, K, C, X, M_, Y_pre, Y_post, A, T = joblib.load(\n",
    "    os.path.join(os.getcwd(), f\"data/diabetes/hp_search.joblib\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonyz\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/Users/jasonyz/Documents/McLatte/.venv.mclatte/lib/python3.9/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jasonyz/mclatte-test/runs/4lxnga2w\" target=\"_blank\">cosmic-dust-1087</a></strong> to <a href=\"https://wandb.ai/jasonyz/mclatte-test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/jasonyz/mclatte-test/runs/4lxnga2w?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa0c839d670>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='mclatte-test', entity='jasonyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def na_catcher(func):\n",
    "    def wrapper_na_catcher(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return np.nan\n",
    "    return wrapper_na_catcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McLatte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_mcespresso(trained_mcespresso, X_test, A_test, T_test, M_test):\n",
    "    trained_mcespresso.eval()\n",
    "    return trained_mcespresso(\n",
    "        torch.from_numpy(X_test).float(),\n",
    "        torch.from_numpy(A_test).float(),\n",
    "        torch.from_numpy(T_test).float(),\n",
    "        torch.from_numpy(M_test).float(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.read_csv(os.path.join(os.getcwd(), 'results/mclatte_hp.csv')).sort_values(by='valid_loss').iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclatte_config = {\n",
    "    'encoder_class': 'lstm',\n",
    "    'decoder_class': 'lstm',\n",
    "    'hidden_dim': 8,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'lr': 0.021089,\n",
    "    'gamma': 0.541449,\n",
    "    'lambda_r': 0.814086,\n",
    "    'lambda_d': 0.185784,\n",
    "    'lambda_p': 0.081336,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mclatte(\n",
    "    X_train, \n",
    "    X_test, \n",
    "    M_train, \n",
    "    M_test, \n",
    "    Y_pre_train, \n",
    "    Y_post_train, \n",
    "    Y_post_test, \n",
    "    A_train, \n",
    "    A_test, \n",
    "    T_train, \n",
    "    T_test,\n",
    "    run_idx=0,\n",
    "):\n",
    "    trained_mclatte = train_mclatte(\n",
    "        mclatte_config,\n",
    "        X_train,\n",
    "        M_train,\n",
    "        Y_pre_train,\n",
    "        Y_post_train,\n",
    "        A_train, \n",
    "        T_train,\n",
    "        R,\n",
    "        M,\n",
    "        H,\n",
    "        input_dim=X_train.shape[2], \n",
    "        treatment_dim=A_train.shape[1], \n",
    "        test_run=run_idx,\n",
    "    )\n",
    "    _, _, y_tilde = infer_mcespresso(\n",
    "        trained_mclatte, X_test, A_test, T_test, M_test\n",
    "    )\n",
    "    \n",
    "    return torch.nn.functional.l1_loss(\n",
    "        y_tilde, \n",
    "        torch.from_numpy(Y_post_test).float()\n",
    "    ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi-Skimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.read_csv(os.path.join(os.getcwd(), 'results/semi_skimmed_mclatte_hp.csv')).sort_values(by='valid_loss').iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "semi_skimmed_mclatte_config = {\n",
    "    'encoder_class': 'lstm',\n",
    "    'decoder_class': 'lstm',\n",
    "    'hidden_dim': 4,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'lr': 0.006606,\n",
    "    'gamma': 0.860694,\n",
    "    'lambda_r': 79.016676,\n",
    "    'lambda_d': 1.2907,\n",
    "    'lambda_p': 11.112241,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_semi_skimmed_mclatte(\n",
    "    X_train, \n",
    "    X_test, \n",
    "    M_train, \n",
    "    M_test, \n",
    "    Y_pre_train, \n",
    "    Y_post_train, \n",
    "    Y_post_test, \n",
    "    A_train, \n",
    "    A_test, \n",
    "    T_train, \n",
    "    T_test,\n",
    "    run_idx=0,\n",
    "):\n",
    "    trained_semi_skimmed_mclatte = train_semi_skimmed_mclatte(\n",
    "        semi_skimmed_mclatte_config,\n",
    "        X_train,\n",
    "        M_train,\n",
    "        Y_pre_train,\n",
    "        Y_post_train,\n",
    "        A_train, \n",
    "        T_train,\n",
    "        R,\n",
    "        M,\n",
    "        H,\n",
    "        input_dim=X_train.shape[2], \n",
    "        treatment_dim=A_train.shape[1], \n",
    "        test_run=run_idx,\n",
    "    )\n",
    "    _, _, y_tilde = infer_mcespresso(\n",
    "        trained_semi_skimmed_mclatte, X_test, A_test, T_test, M_test\n",
    "    )\n",
    "    \n",
    "    return torch.nn.functional.l1_loss(\n",
    "        y_tilde, \n",
    "        torch.from_numpy(Y_post_test).float()\n",
    "    ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.read_csv(os.path.join(os.getcwd(), 'results/skimmed_mclatte_hp.csv')).sort_values(by='valid_loss').iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimmed_mclatte_config = {\n",
    "    'encoder_class': 'lstm',\n",
    "    'decoder_class': 'lstm',\n",
    "    'hidden_dim': 16,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'lr': 0.000928,\n",
    "    'gamma': 0.728492,\n",
    "    'lambda_r': 1.100493,\n",
    "    'lambda_p': 2.108935,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_skimmed_mclatte(\n",
    "    X_train, \n",
    "    X_test, \n",
    "    M_train, \n",
    "    M_test, \n",
    "    Y_pre_train, \n",
    "    Y_post_train, \n",
    "    Y_post_test, \n",
    "    A_train, \n",
    "    A_test, \n",
    "    T_train, \n",
    "    T_test,\n",
    "    run_idx=0,\n",
    "):\n",
    "    trained_skimmed_mclatte = train_skimmed_mclatte(\n",
    "        skimmed_mclatte_config,\n",
    "        X_train,\n",
    "        M_train,\n",
    "        Y_pre_train,\n",
    "        Y_post_train,\n",
    "        A_train, \n",
    "        T_train,\n",
    "        R,\n",
    "        M,\n",
    "        H,\n",
    "        input_dim=X_train.shape[2], \n",
    "        treatment_dim=A_train.shape[1], \n",
    "        test_run=run_idx,\n",
    "    )\n",
    "    _, y_tilde = infer_mcespresso(\n",
    "        trained_skimmed_mclatte, X_test, A_test, T_test, M_test\n",
    "    )\n",
    "    \n",
    "    return torch.nn.functional.l1_loss(\n",
    "        y_tilde, \n",
    "        torch.from_numpy(Y_post_test).float()\n",
    "    ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.read_csv(os.path.join(os.getcwd(), 'results/baseline_rnn_hp.csv')).sort_values(by='valid_loss').iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_config = {\n",
    "    'rnn_class': 'gru',\n",
    "    'hidden_dim': 64,\n",
    "    'seq_len': 2,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 100,\n",
    "    'lr': 0.006321,\n",
    "    'gamma': 0.543008,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(trained_rnn, Y_pre, Y_post, return_Y_pred=False):\n",
    "    \"\"\"\n",
    "    Make predictions using results from previous time steps.\n",
    "    \"\"\"\n",
    "    Y = Y_pre\n",
    "    losses = 0.0\n",
    "    Y_pred = []\n",
    "    for i in range(Y_post.shape[1]):\n",
    "        Y_tilde = trained_rnn(\n",
    "            torch.from_numpy(Y).float().unsqueeze(2)\n",
    "        ).squeeze()\n",
    "\n",
    "        Y = np.concatenate((\n",
    "            Y[:, 1:], \n",
    "            Y_tilde.cpu().detach().numpy()[:, [-1]]\n",
    "        ), axis=1)\n",
    "        \n",
    "        losses += torch.nn.functional.l1_loss(\n",
    "            Y_tilde[:, -1], \n",
    "            torch.from_numpy(Y_post).float()[:, i]\n",
    "        ).item()\n",
    "        Y_pred.append(Y_tilde[:, -1])\n",
    "    if return_Y_pred:\n",
    "        return torch.stack(Y_pred, 1)\n",
    "    return losses / Y_post.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_rnn(trained_rnn, Y_pre_test, Y_post_test, return_Y_pred=False):\n",
    "    trained_rnn.eval()\n",
    "    return rnn_predict(trained_rnn, Y_pre_test, Y_post_test, return_Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn(\n",
    "    Y_pre_train, \n",
    "    Y_pre_test, \n",
    "    Y_post_train, \n",
    "    Y_post_test, \n",
    "    run_idx=0,\n",
    "):\n",
    "    trained_rnn = train_baseline_rnn(\n",
    "        rnn_config,\n",
    "        Y=np.concatenate((Y_pre_train, Y_post_train), axis=1),\n",
    "        input_dim=1, \n",
    "        test_run=run_idx,\n",
    "    )\n",
    "\n",
    "    trained_rnn.eval()\n",
    "    return rnn_predict(trained_rnn, Y_pre_test, Y_post_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SyncTwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.read_csv(os.path.join(os.getcwd(), 'results/synctwin_hp.csv')).sort_values(by='valid_loss').iloc[0])\n",
    "synctwin_config = {\n",
    "    'hidden_dim': 128,\n",
    "    'reg_B': 0.522652,\n",
    "    'lam_express': 0.163847,\n",
    "    'lam_recon': 0.39882,\n",
    "    'lam_prognostic': 0.837303,\n",
    "    'tau': 0.813696,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'lr': 0.001476,\n",
    "    'gamma': 0.912894,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_synctwin(trained_synctwin, N_test, Y_post_test):\n",
    "    trained_synctwin.eval()\n",
    "    return trained_synctwin._sync_twin.get_prognostics(\n",
    "        torch.arange(0, N_test).cpu(),  \n",
    "        torch.from_numpy(Y_post_test).float().cpu()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_synctwin(\n",
    "    N_train, \n",
    "    N_test, \n",
    "    X_train, \n",
    "    X_test, \n",
    "    M_train, \n",
    "    M_test, \n",
    "    Y_post_train, \n",
    "    Y_post_test, \n",
    "    A_train, \n",
    "    A_test, \n",
    "    T_train, \n",
    "    T_test,\n",
    "    run_idx=0,\n",
    "):\n",
    "    Y_mask_train = np.all(A_train == 0, axis=1)\n",
    "    Y_mask_test = np.all(A_test == 0, axis=1)\n",
    "    Y_control_train = Y_post_train[Y_mask_train]\n",
    "\n",
    "    trained_synctwin = train_synctwin(\n",
    "        synctwin_config,\n",
    "        X=X_train,\n",
    "        M_=M_train,\n",
    "        T=T_train,\n",
    "        Y_batch=Y_post_train,\n",
    "        Y_control=Y_control_train,\n",
    "        Y_mask=Y_mask_train, \n",
    "        N=N_train,\n",
    "        D=X_train.shape[2],\n",
    "        n_treated=N_train - Y_control_train.shape[0],\n",
    "        pre_trt_x_len=R * M,\n",
    "        test_run=run_idx,\n",
    "    )\n",
    "\n",
    "    trained_synctwin.eval()\n",
    "    _, l1_loss = trained_synctwin(\n",
    "        torch.from_numpy(X_test).float(),\n",
    "        torch.from_numpy(T_test).float(),\n",
    "        torch.from_numpy(M_test).float(),\n",
    "        torch.arange(0, N_test),\n",
    "        torch.from_numpy(Y_post_test).float(),\n",
    "        torch.from_numpy(Y_mask_test).float(),\n",
    "    )\n",
    "    return l1_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    mclatte_losses = []\n",
    "    semi_skimmed_mclatte_losses = []\n",
    "    skimmed_mclatte_losses = []\n",
    "    rnn_losses = []\n",
    "    synctwin_losses = []\n",
    "    for i in range(1, N_TEST + 1):\n",
    "        (\n",
    "            _, \n",
    "            N_train, \n",
    "            N_test, \n",
    "            X_train, \n",
    "            X_test, \n",
    "            M_train, \n",
    "            M_test, \n",
    "            Y_pre_train, \n",
    "            Y_pre_test, \n",
    "            Y_post_train, \n",
    "            Y_post_test, \n",
    "            A_train, \n",
    "            A_test, \n",
    "            T_train, \n",
    "            T_test,\n",
    "        ) = generate_data()\n",
    "\n",
    "        mclatte_losses.append(test_mclatte(\n",
    "            X_train, \n",
    "            X_test, \n",
    "            M_train, \n",
    "            M_test, \n",
    "            Y_pre_train, \n",
    "            Y_post_train, \n",
    "            Y_post_test, \n",
    "            A_train, \n",
    "            A_test, \n",
    "            T_train, \n",
    "            T_test,\n",
    "            run_idx=i,\n",
    "        ))\n",
    "        semi_skimmed_mclatte_losses.append(test_semi_skimmed_mclatte(\n",
    "            X_train, \n",
    "            X_test, \n",
    "            M_train, \n",
    "            M_test, \n",
    "            Y_pre_train, \n",
    "            Y_post_train, \n",
    "            Y_post_test, \n",
    "            A_train, \n",
    "            A_test, \n",
    "            T_train, \n",
    "            T_test,\n",
    "            run_idx=i,\n",
    "        ))\n",
    "        skimmed_mclatte_losses.append(test_skimmed_mclatte(\n",
    "            X_train, \n",
    "            X_test, \n",
    "            M_train, \n",
    "            M_test, \n",
    "            Y_pre_train, \n",
    "            Y_post_train, \n",
    "            Y_post_test, \n",
    "            A_train, \n",
    "            A_test, \n",
    "            T_train, \n",
    "            T_test,\n",
    "            run_idx=i,\n",
    "        ))\n",
    "\n",
    "        rnn_losses.append(test_rnn(\n",
    "            Y_pre_train, \n",
    "            Y_pre_test, \n",
    "            Y_post_train, \n",
    "            Y_post_test, \n",
    "            run_idx=i,\n",
    "        ))\n",
    "\n",
    "        \"\"\"\n",
    "        synctwin_losses.append(test_synctwin(\n",
    "            N_train, \n",
    "            N_test, \n",
    "            X_train, \n",
    "            X_test, \n",
    "            M_train, \n",
    "            M_test, \n",
    "            Y_post_train, \n",
    "            Y_post_test, \n",
    "            A_train, \n",
    "            A_test, \n",
    "            T_train, \n",
    "            T_test,\n",
    "            run_idx=i,\n",
    "        ))\n",
    "        \"\"\"\n",
    "        \n",
    "        joblib.dump((\n",
    "            mclatte_losses, \n",
    "            semi_skimmed_mclatte_losses, \n",
    "            skimmed_mclatte_losses, \n",
    "            rnn_losses,\n",
    "            synctwin_losses,\n",
    "        ), f'results/test/diabetes.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type        | Params\n",
      "-----------------------------------------\n",
      "0 | _encoder | LstmEncoder | 808   \n",
      "1 | _decoder | LstmDecoder | 684   \n",
      "-----------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jasonyz/Documents/McLatte/results/mclatte_1.ckpt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type        | Params\n",
      "-----------------------------------------\n",
      "0 | _encoder | LstmEncoder | 808   \n",
      "1 | _decoder | LstmDecoder | 684   \n",
      "-----------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jasonyz/Documents/McLatte/results/mclatte_2.ckpt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type        | Params\n",
      "-----------------------------------------\n",
      "0 | _encoder | LstmEncoder | 808   \n",
      "1 | _decoder | LstmDecoder | 684   \n",
      "-----------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jasonyz/Documents/McLatte/results/mclatte_3.ckpt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type        | Params\n",
      "-----------------------------------------\n",
      "0 | _encoder | LstmEncoder | 808   \n",
      "1 | _decoder | LstmDecoder | 684   \n",
      "-----------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jasonyz/Documents/McLatte/results/mclatte_4.ckpt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name     | Type        | Params\n",
      "-----------------------------------------\n",
      "0 | _encoder | LstmEncoder | 808   \n",
      "1 | _decoder | LstmDecoder | 684   \n",
      "-----------------------------------------\n",
      "1.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/Users/jasonyz/Documents/McLatte/results/mclatte_5.ckpt'\n"
     ]
    }
   ],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check finished runs results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_losses():\n",
    "    all_losses = joblib.load(f'results_diabetes/maes/diabetes.joblib')\n",
    "    for losses in all_losses:\n",
    "        print(f'{np.mean(losses):.3f} ({np.std(losses):.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920 (0.015)\n",
      "0.903 (0.018)\n",
      "0.906 (0.001)\n",
      "0.882 (0.014)\n",
      "nan (nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonyz/Documents/McLatte/.venv.mclatte/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jasonyz/Documents/McLatte/.venv.mclatte/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/jasonyz/Documents/McLatte/.venv.mclatte/lib/python3.9/site-packages/numpy/core/_methods.py:262: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/jasonyz/Documents/McLatte/.venv.mclatte/lib/python3.9/site-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/Users/jasonyz/Documents/McLatte/.venv.mclatte/lib/python3.9/site-packages/numpy/core/_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "print_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_NAMES = ['McLatte', 'Semi-Skimmed McLatte', 'Skimmed McLatte', 'RNN', 'SyncTwin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_losses(losses):\n",
    "    t_test_results = pd.DataFrame(columns=LOSS_NAMES, index=LOSS_NAMES)\n",
    "\n",
    "    for i in range(len(losses)):\n",
    "        for j in range(len(losses)):\n",
    "            t = ttest_ind(losses[i], losses[j], alternative='less')\n",
    "            t_test_results[LOSS_NAMES[i]][LOSS_NAMES[j]] = t.pvalue\n",
    "    return t_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>McLatte</th>\n",
       "      <th>Semi-Skimmed McLatte</th>\n",
       "      <th>Skimmed McLatte</th>\n",
       "      <th>RNN</th>\n",
       "      <th>SyncTwin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>McLatte</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.086596</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semi-Skimmed McLatte</th>\n",
       "      <td>0.913404</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.637164</td>\n",
       "      <td>0.046576</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skimmed McLatte</th>\n",
       "      <td>0.950748</td>\n",
       "      <td>0.362836</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RNN</th>\n",
       "      <td>0.997353</td>\n",
       "      <td>0.953424</td>\n",
       "      <td>0.996426</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SyncTwin</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       McLatte Semi-Skimmed McLatte Skimmed McLatte       RNN  \\\n",
       "McLatte                    0.5             0.086596        0.049252  0.002647   \n",
       "Semi-Skimmed McLatte  0.913404                  0.5        0.637164  0.046576   \n",
       "Skimmed McLatte       0.950748             0.362836             0.5  0.003574   \n",
       "RNN                   0.997353             0.953424        0.996426       0.5   \n",
       "SyncTwin                   NaN                  NaN             NaN       NaN   \n",
       "\n",
       "                     SyncTwin  \n",
       "McLatte                   NaN  \n",
       "Semi-Skimmed McLatte      NaN  \n",
       "Skimmed McLatte           NaN  \n",
       "RNN                       NaN  \n",
       "SyncTwin                  NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = joblib.load(f'results_diabetes/maes/diabetes.joblib')[:4]\n",
    "test_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_NAME = {\n",
    "    'skimmed_mclatte': 'S',\n",
    "    'semi_skimmed_mclatte': 'SS',\n",
    "    'mclatte': 'V',\n",
    "    'rnn': 'RNN',\n",
    "    'synctwin': 'SyncTwin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_model_pred(fig, model, name, infer_model, plot_sub_id, post_t, y_pre_plot, file_suffix, *infer_args):\n",
    "    trained_model = model.load_from_checkpoint(os.path.join(os.getcwd(), f'results_idt/trained_models/{name}.ckpt'))\n",
    "    y_tilde = infer_model(trained_model, *infer_args)\n",
    "    y_pred_plot = [y_pre_plot[-1]] + list(y_tilde.detach().numpy()[plot_sub_id])\n",
    "    line_pred_model = go.Scatter(x=post_t, y=y_pred_plot, name=f'{PLOT_NAME[name]}{file_suffix}', line={'dash': 'dash'})\n",
    "    fig.add_trace(line_pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_subject(fig, plot_sub_id, N_test, X_test, M_test, post_t, y_pre_plot, A_test, T_test, Y_pre_test, Y_post_test, file_suffix=''):\n",
    "    line_model_pred(fig, SkimmedMcLatte, 'skimmed_mclatte', lambda *args: infer_mcespresso(*args)[1], \n",
    "                    plot_sub_id, post_t, y_pre_plot, file_suffix, X_test, A_test, T_test, M_test)\n",
    "    line_model_pred(fig, SemiSkimmedMcLatte, 'semi_skimmed_mclatte', lambda *args: infer_mcespresso(*args)[2], \n",
    "                    plot_sub_id, post_t, y_pre_plot, file_suffix, X_test, A_test, T_test, M_test)\n",
    "    line_model_pred(fig, McLatte, 'mclatte', lambda *args: infer_mcespresso(*args)[2], \n",
    "                    plot_sub_id, post_t, y_pre_plot, file_suffix, X_test, A_test, T_test, M_test)\n",
    "    line_model_pred(fig, BaselineRnn, 'rnn', lambda *args: infer_rnn(*args, return_Y_pred=True), \n",
    "                    plot_sub_id, post_t, y_pre_plot, file_suffix, Y_pre_test, Y_post_test)\n",
    "    line_model_pred(fig, SyncTwinPl, 'synctwin', lambda *args: infer_synctwin(*args), \n",
    "                    plot_sub_id, post_t, y_pre_plot, file_suffix, N_test, Y_post_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_config_results(file_suffix=''):\n",
    "    (\n",
    "        _, _, N_test, \n",
    "        _, X_test, \n",
    "        _, M_test, \n",
    "        _, Y_pre_test, \n",
    "        _, Y_post_test, \n",
    "        _, A_test, \n",
    "        _, T_test,\n",
    "    ) = generate_data()\n",
    "\n",
    "    for plot_sub_id in range(N_test):\n",
    "        y_pre_plot = Y_pre_test[plot_sub_id]\n",
    "        pre_t = list(np.arange(y_pre_plot.shape[0]) - y_pre_plot.shape[0])\n",
    "\n",
    "        y_post_plot = [y_pre_plot[-1]] + list(Y_post_test[plot_sub_id])\n",
    "        post_t = np.arange(len(y_post_plot))\n",
    "        \n",
    "        trt_str = ', '.join(map(\n",
    "            lambda x: str(round(x, 2)) if abs(x - round(x)) > 5e-2 else str(int(x)), \n",
    "            A_test[plot_sub_id]\n",
    "        ))\n",
    "\n",
    "        fig = go.Figure()\n",
    "        line_pre_trt = go.Scatter(x=pre_t + list(post_t), y=list(y_pre_plot) + y_post_plot, name='ground truth')\n",
    "        fig.add_trace(line_pre_trt)\n",
    "\n",
    "        plot_subject(fig, plot_sub_id, N_test, X_test, M_test, post_t, y_pre_plot, A_test, T_test, Y_pre_test, Y_post_test)\n",
    "        A_test[plot_sub_id] = np.ones_like(A_test[plot_sub_id]) if not (A_test[plot_sub_id] == 0).all() else np.zeros_like(A_test[plot_sub_id])\n",
    "        plot_subject(fig, plot_sub_id, N_test, X_test, M_test, post_t, y_pre_plot, A_test, T_test, Y_pre_test, Y_post_test, file_suffix=' 01')\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Outcome for Treatment Vector ({trt_str})', \n",
    "            yaxis_title='Outcome', \n",
    "            xaxis_title='Time',\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ),\n",
    "        )\n",
    "        fig.write_image(f'plots/diabetes/outcome_pred_{plot_sub_id}{file_suffix}.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "852d66bd2a456ad8c66b277a4fc8ac1a7bcd6ab870e3be824a8a6385faf13d3b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv.mclatte': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
